{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bd7c75",
   "metadata": {},
   "source": [
    "# Motor Fault Detection using Eulerian Video Magnification\n",
    "\n",
    "This notebook implements a complete pipeline for detecting motor faults using Eulerian Video Magnification (EVM). The pipeline processes video data to extract and magnify subtle motions, then classifies the motor state as Normal or Abnormal using machine learning.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. Frame Extraction - Extract and resample video frames\n",
    "2. Video Stabilization - Remove camera shake\n",
    "3. ROI Detection - Locate the region of interest\n",
    "4. Motion Magnification - Amplify subtle motions using EVM\n",
    "5. Feature Extraction - Compute motion and frequency features\n",
    "6. Classification - Train SVM model to detect faults\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset consists of:\n",
    "- Normal.mov - 24-minute video of normal motor operation\n",
    "- Abnormal.mov - 24-minute video of motor with fault\n",
    "- Test videos for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c618014",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, we'll install all the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bd4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless numpy scipy scikit-image scikit-learn pyrtools tqdm joblib gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196b9fb",
   "metadata": {},
   "source": [
    "## 2. Download Dataset from Google Drive\n",
    "\n",
    "Mount Google Drive and verify access to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Dataset folder path\n",
    "DATASET_PATH = \"/content/drive/MyDrive/1hOu0PExCWjvA5H5cXz7dBS0JNYW5sM7Z\"\n",
    "NORMAL_VIDEO = os.path.join(DATASET_PATH, \"Normal.mov\")\n",
    "ABNORMAL_VIDEO = os.path.join(DATASET_PATH, \"Abnormal.mov\")\n",
    "\n",
    "print(\"Checking dataset files...\")\n",
    "for file_path in [NORMAL_VIDEO, ABNORMAL_VIDEO]:\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Found {os.path.basename(file_path)}\")\n",
    "    else:\n",
    "        print(f\"Missing {os.path.basename(file_path)}\")\n",
    "\n",
    "# Get list of test videos\n",
    "test_videos = [f for f in os.listdir(DATASET_PATH) if f not in [\"Normal.mov\", \"Abnormal.mov\"]]\n",
    "print(f\"\\nFound {len(test_videos)} test videos:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fdb2d",
   "metadata": {},
   "source": [
    "## 3. Frame Extraction\n",
    "\n",
    "Define and test the function to extract frames from videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7348ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path: str, fps: int = 30, max_frames: int = None) -> List[np.ndarray]:\n",
    "    \"\"\"Extract frames from video at specified FPS.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    video_path : str\n",
    "        Path to video file\n",
    "    fps : int\n",
    "        Target frames per second\n",
    "    max_frames : int, optional\n",
    "        Maximum number of frames to extract\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[np.ndarray]\n",
    "        List of RGB frames\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open {video_path}\")\n",
    "    \n",
    "    orig_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    with tqdm(desc=\"Extracting frames\") as pbar:\n",
    "        success, frame = cap.read()\n",
    "        while success:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if max_frames and frame_count >= max_frames:\n",
    "                break\n",
    "            success, frame = cap.read()\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if not frames:\n",
    "        return frames\n",
    "    \n",
    "    # Resample to target FPS if needed\n",
    "    if abs(orig_fps - fps) > 1e-2:\n",
    "        n_frames_out = max(1, int(round(len(frames) * fps / orig_fps)))\n",
    "        indices = np.linspace(0, len(frames) - 1, n_frames_out).astype(int)\n",
    "        frames = [frames[i] for i in indices]\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Test frame extraction (using first 100 frames)\n",
    "test_frames = extract_frames(NORMAL_VIDEO, fps=30, max_frames=100)\n",
    "print(f\"Extracted {len(test_frames)} frames of shape {test_frames[0].shape}\")\n",
    "\n",
    "# Display first frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(test_frames[0])\n",
    "plt.axis('off')\n",
    "plt.title('First Frame from Normal Video')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ff1a6",
   "metadata": {},
   "source": [
    "## 4. Frame Stabilization\n",
    "\n",
    "Implement video stabilization using ORB features and homography:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29821ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilise_frames(frames: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    \"\"\"Stabilize video using feature matching and homography.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frames : List[np.ndarray]\n",
    "        Input RGB frames\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[np.ndarray]\n",
    "        Stabilized frames\n",
    "    \"\"\"\n",
    "    if len(frames) < 2:\n",
    "        return frames\n",
    "    \n",
    "    height, width = frames[0].shape[:2]\n",
    "    orb = cv2.ORB_create(500)\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    transforms = [np.eye(3, dtype=np.float32)]\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    with tqdm(total=len(frames)-1, desc=\"Stabilizing frames\") as pbar:\n",
    "        for idx in range(1, len(frames)):\n",
    "            curr_gray = cv2.cvtColor(frames[idx], cv2.COLOR_RGB2GRAY)\n",
    "            k1, d1 = orb.detectAndCompute(prev_gray, None)\n",
    "            k2, d2 = orb.detectAndCompute(curr_gray, None)\n",
    "            \n",
    "            if d1 is None or d2 is None:\n",
    "                transforms.append(transforms[-1])\n",
    "                prev_gray = curr_gray\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "                \n",
    "            matches = matcher.match(d1, d2)\n",
    "            if len(matches) < 4:\n",
    "                transforms.append(transforms[-1])\n",
    "                prev_gray = curr_gray\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "                \n",
    "            pts1 = np.float32([k1[m.queryIdx].pt for m in matches])\n",
    "            pts2 = np.float32([k2[m.trainIdx].pt for m in matches])\n",
    "            H, mask = cv2.findHomography(pts2, pts1, cv2.RANSAC, 5.0)\n",
    "            \n",
    "            if H is None:\n",
    "                H = transforms[-1]\n",
    "            else:\n",
    "                H = H.astype(np.float32)\n",
    "                \n",
    "            transforms.append(H @ transforms[-1])\n",
    "            prev_gray = curr_gray\n",
    "            pbar.update(1)\n",
    "    \n",
    "    stabilised = []\n",
    "    for frame, H in zip(frames, transforms):\n",
    "        warped = cv2.warpPerspective(frame, H, (width, height),\n",
    "                                   flags=cv2.INTER_LINEAR,\n",
    "                                   borderMode=cv2.BORDER_REFLECT)\n",
    "        stabilised.append(warped)\n",
    "    \n",
    "    return stabilised\n",
    "\n",
    "# Test stabilization\n",
    "stable_frames = stabilise_frames(test_frames)\n",
    "\n",
    "# Show before/after comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.imshow(test_frames[0])\n",
    "ax1.set_title('Before Stabilization')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(stable_frames[0])\n",
    "ax2.set_title('After Stabilization')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ccbd0",
   "metadata": {},
   "source": [
    "## 5. ROI Detection and Cropping\n",
    "\n",
    "Implement ROI detection using the red marker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(frames: List[np.ndarray], marker_hsv: Tuple[int, int, int] = (0, 255, 255), \n",
    "           pad: int = 10) -> List[np.ndarray]:\n",
    "    \"\"\"Detect and crop region around the red marker.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frames : List[np.ndarray]\n",
    "        Input RGB frames\n",
    "    marker_hsv : Tuple[int, int, int]\n",
    "        HSV color of the marker\n",
    "    pad : int\n",
    "        Padding around detected ROI\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[np.ndarray]\n",
    "        Cropped frames\n",
    "    \"\"\"\n",
    "    if not frames:\n",
    "        return frames\n",
    "    \n",
    "    height, width = frames[0].shape[:2]\n",
    "    boxes = []\n",
    "    tol = np.array([10, 100, 100])\n",
    "    hsv_target = np.array(marker_hsv)\n",
    "    \n",
    "    with tqdm(total=len(frames), desc=\"Detecting ROI\") as pbar:\n",
    "        for frame in frames:\n",
    "            hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)\n",
    "            lower = np.maximum(hsv_target - tol, (0, 0, 0))\n",
    "            upper = np.minimum(hsv_target + tol, (179, 255, 255))\n",
    "            mask = cv2.inRange(hsv, lower, upper)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                cnt = max(contours, key=cv2.contourArea)\n",
    "                if cv2.contourArea(cnt) > 0:\n",
    "                    x, y, w, h = cv2.boundingRect(cnt)\n",
    "                    boxes.append((x, y, x + w, y + h))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    if not boxes:\n",
    "        print(\"[WARN] Marker not found; returning full frames\")\n",
    "        return frames\n",
    "    \n",
    "    box = np.array(boxes)\n",
    "    x0, y0 = box[:, :2].min(axis=0)\n",
    "    x1, y1 = box[:, 2:].max(axis=0)\n",
    "    \n",
    "    x0 = max(0, x0 - pad)\n",
    "    y0 = max(0, y0 - pad)\n",
    "    x1 = min(width, x1 + pad)\n",
    "    y1 = min(height, y1 + pad)\n",
    "    \n",
    "    cropped = [f[y0:y1, x0:x1].copy() for f in frames]\n",
    "    return cropped\n",
    "\n",
    "# Test ROI detection\n",
    "cropped_frames = crop_roi(stable_frames)\n",
    "print(f\"Original shape: {stable_frames[0].shape}\")\n",
    "print(f\"Cropped shape: {cropped_frames[0].shape}\")\n",
    "\n",
    "# Show before/after comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.imshow(stable_frames[0])\n",
    "ax1.set_title('Before ROI Cropping')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(cropped_frames[0])\n",
    "ax2.set_title('After ROI Cropping')\n",
    "ax2.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94c41c",
   "metadata": {},
   "source": [
    "## 6. Laplacian Pyramid\n",
    "\n",
    "Implement pyramid construction and reconstruction for motion magnification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01419e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_laplacian_pyramid(img: np.ndarray, levels: int = 5) -> List[np.ndarray]:\n",
    "    \"\"\"Build Laplacian pyramid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        Input RGB image\n",
    "    levels : int\n",
    "        Number of pyramid levels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[np.ndarray]\n",
    "        Pyramid levels\n",
    "    \"\"\"\n",
    "    pyr = []\n",
    "    current = img.astype(np.float32)\n",
    "    \n",
    "    for _ in range(levels):\n",
    "        down = cv2.pyrDown(current)\n",
    "        up = cv2.pyrUp(down, dstsize=(current.shape[1], current.shape[0]))\n",
    "        lap = current - up\n",
    "        pyr.append(lap)\n",
    "        current = down\n",
    "    \n",
    "    pyr.append(current)\n",
    "    return pyr\n",
    "\n",
    "def reconstruct_from_laplacian(pyr: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Reconstruct image from Laplacian pyramid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pyr : List[np.ndarray]\n",
    "        Pyramid levels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Reconstructed image\n",
    "    \"\"\"\n",
    "    current = pyr[-1]\n",
    "    for lap in reversed(pyr[:-1]):\n",
    "        up = cv2.pyrUp(current, dstsize=(lap.shape[1], lap.shape[0]))\n",
    "        current = up + lap\n",
    "    \n",
    "    current = np.clip(current, 0, 255)\n",
    "    return current.astype(np.uint8)\n",
    "\n",
    "# Test pyramid construction and reconstruction\n",
    "test_frame = cropped_frames[0]\n",
    "pyr = build_laplacian_pyramid(test_frame, levels=4)\n",
    "recon = reconstruct_from_laplacian(pyr)\n",
    "\n",
    "# Compute PSNR\n",
    "mse = np.mean((test_frame.astype(np.float32) - recon.astype(np.float32)) ** 2)\n",
    "psnr = 10 * np.log10(255**2 / (mse + 1e-8))\n",
    "print(f\"Reconstruction PSNR: {psnr:.2f} dB\")\n",
    "\n",
    "# Display pyramid levels and reconstruction\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Laplacian Pyramid and Reconstruction')\n",
    "\n",
    "for i, level in enumerate(pyr[:-1]):\n",
    "    ax = axs.flat[i]\n",
    "    ax.imshow(np.abs(level).astype(np.uint8))\n",
    "    ax.set_title(f'Level {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "axs.flat[-2].imshow(pyr[-1].astype(np.uint8))\n",
    "axs.flat[-2].set_title('Gaussian Residual')\n",
    "axs.flat[-2].axis('off')\n",
    "\n",
    "axs.flat[-1].imshow(recon)\n",
    "axs.flat[-1].set_title('Reconstruction')\n",
    "axs.flat[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c0e73",
   "metadata": {},
   "source": [
    "## 7. Temporal Filtering\n",
    "\n",
    "Implement temporal bandpass filter for motion isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ac342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_bandpass(stack: np.ndarray, fs: int, f_low: float, f_high: float, \n",
    "                      order: int = 3) -> np.ndarray:\n",
    "    \"\"\"Apply temporal bandpass filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stack : np.ndarray\n",
    "        Input array with time as first dimension\n",
    "    fs : int\n",
    "        Sampling frequency in Hz\n",
    "    f_low : float\n",
    "        Low cutoff frequency\n",
    "    f_high : float\n",
    "        High cutoff frequency\n",
    "    order : int\n",
    "        Filter order\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Filtered array\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = f_low / nyq\n",
    "    high = f_high / nyq\n",
    "    b, a = butter(order, [low, high], btype=\"band\")\n",
    "    \n",
    "    T = stack.shape[0]\n",
    "    flat = stack.reshape(T, -1)\n",
    "    filtered = filtfilt(b, a, flat, axis=0)\n",
    "    return filtered.reshape(stack.shape)\n",
    "\n",
    "def amplify(stack: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    \"\"\"Amplify signal by constant factor.\"\"\"\n",
    "    return stack * alpha\n",
    "\n",
    "# Test temporal filtering\n",
    "fs = 30  # Hz\n",
    "t = np.arange(0, 1, 1/fs)\n",
    "freq = 3.0  # target frequency\n",
    "\n",
    "# Create synthetic motion signal\n",
    "signal = np.sin(2 * np.pi * freq * t) + 0.5 * np.random.randn(len(t))\n",
    "stack = signal[:, None, None].astype(np.float32)\n",
    "\n",
    "# Apply filter\n",
    "filtered = temporal_bandpass(stack, fs, freq-0.5, freq+0.5).squeeze()\n",
    "\n",
    "# Plot frequency domain comparison\n",
    "fft_freqs = np.fft.rfftfreq(len(t), 1/fs)\n",
    "orig_fft = np.abs(np.fft.rfft(signal))\n",
    "filt_fft = np.abs(np.fft.rfft(filtered))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(fft_freqs, orig_fft, label='Original')\n",
    "plt.plot(fft_freqs, filt_fft, label='Filtered')\n",
    "plt.axvline(freq, color='r', linestyle='--', label='Target frequency')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Frequency Response of Bandpass Filter')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print amplitude at target frequency\n",
    "target_idx = np.argmin(np.abs(fft_freqs - freq))\n",
    "print(f\"Amplitude at {freq} Hz before filtering: {orig_fft[target_idx]:.3f}\")\n",
    "print(f\"Amplitude at {freq} Hz after filtering: {filt_fft[target_idx]:.3f}\")\n",
    "print(f\"Peak frequency in filtered signal: {fft_freqs[np.argmax(filt_fft)]:.2f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a56d30",
   "metadata": {},
   "source": [
    "## 8. Motion Magnification\n",
    "\n",
    "Combine pyramid decomposition and temporal filtering to magnify motion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520aa9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnify_motion(frames: List[np.ndarray], fs: int, f_low: float, f_high: float,\n",
    "                 alpha: float, levels: int = 5) -> List[np.ndarray]:\n",
    "    \"\"\"Magnify motion in video.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frames : List[np.ndarray]\n",
    "        Input RGB frames\n",
    "    fs : int\n",
    "        Frame rate in Hz\n",
    "    f_low : float\n",
    "        Low cutoff frequency\n",
    "    f_high : float\n",
    "        High cutoff frequency\n",
    "    alpha : float\n",
    "        Amplification factor\n",
    "    levels : int\n",
    "        Number of pyramid levels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[np.ndarray]\n",
    "        Motion magnified frames\n",
    "    \"\"\"\n",
    "    if not frames:\n",
    "        return []\n",
    "    \n",
    "    # Build Laplacian pyramids\n",
    "    print(\"Building Laplacian pyramids...\")\n",
    "    pyramids = [build_laplacian_pyramid(f, levels) for f in frames]\n",
    "    n_levels = len(pyramids[0])\n",
    "    T = len(frames)\n",
    "    \n",
    "    # Process each level\n",
    "    print(\"Processing pyramid levels...\")\n",
    "    for lvl in tqdm(range(n_levels), desc=\"Magnifying motion\"):\n",
    "        stack = np.stack([pyr[lvl] for pyr in pyramids], axis=0)\n",
    "        filtered = temporal_bandpass(stack, fs, f_low, f_high)\n",
    "        amplified = amplify(filtered, alpha)\n",
    "        \n",
    "        for t in range(T):\n",
    "            pyramids[t][lvl] = pyramids[t][lvl] + amplified[t]\n",
    "    \n",
    "    # Reconstruct frames\n",
    "    print(\"Reconstructing frames...\")\n",
    "    out_frames = [reconstruct_from_laplacian(pyr) for pyr in pyramids]\n",
    "    return out_frames\n",
    "\n",
    "# Test motion magnification on a short sequence\n",
    "magnified = magnify_motion(cropped_frames[:100], fs=30, \n",
    "                         f_low=0.5, f_high=3.0, alpha=10)\n",
    "\n",
    "# Display original vs magnified frames\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.imshow(cropped_frames[0])\n",
    "ax1.set_title('Original Frame')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(magnified[0])\n",
    "ax2.set_title('Motion Magnified Frame')\n",
    "ax2.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Save a few frames as GIF for visualization\n",
    "from IPython.display import Image\n",
    "import imageio\n",
    "\n",
    "print(\"Creating preview GIF...\")\n",
    "frames_for_gif = [frame[::2, ::2] for frame in magnified[:30]]  # Downsample for size\n",
    "imageio.mimsave('magnified_preview.gif', frames_for_gif, fps=10)\n",
    "display(Image('magnified_preview.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5afdd5",
   "metadata": {},
   "source": [
    "## 9. Feature Extraction\n",
    "\n",
    "Extract and combine energy, optical flow, and FFT features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_features(frames: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"Compute energy features from frame differences.\"\"\"\n",
    "    if len(frames) < 2:\n",
    "        return np.zeros(3, dtype=np.float32)\n",
    "    \n",
    "    diffs = []\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "    \n",
    "    for f in frames[1:]:\n",
    "        gray = cv2.cvtColor(f, cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "        diff = np.abs(gray - prev_gray)\n",
    "        diffs.append(diff)\n",
    "        prev_gray = gray\n",
    "    \n",
    "    diffs = np.stack(diffs, axis=0)\n",
    "    feat = np.array([\n",
    "        diffs.mean(),\n",
    "        diffs.std(),\n",
    "        diffs.max()\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    return feat\n",
    "\n",
    "def flow_hist_features(frames: List[np.ndarray], bins: int = 16) -> np.ndarray:\n",
    "    \"\"\"Compute optical flow histogram features.\"\"\"\n",
    "    if len(frames) < 2:\n",
    "        return np.zeros(bins, dtype=np.float32)\n",
    "    \n",
    "    # Resize frames for faster processing\n",
    "    target_h = 160\n",
    "    resized = [cv2.resize(f, (int(f.shape[1] * target_h / f.shape[0]), target_h))\n",
    "               for f in frames]\n",
    "    \n",
    "    mags = []\n",
    "    max_mag = 0.0\n",
    "    for i in range(len(resized) - 1):\n",
    "        g1 = cv2.cvtColor(resized[i], cv2.COLOR_RGB2GRAY)\n",
    "        g2 = cv2.cvtColor(resized[i + 1], cv2.COLOR_RGB2GRAY)\n",
    "        flow = cv2.calcOpticalFlowFarneback(\n",
    "            g1, g2, None,\n",
    "            pyr_scale=0.5, levels=3, winsize=15,\n",
    "            iterations=3, poly_n=5, poly_sigma=1.2, flags=0,\n",
    "        )\n",
    "        mag = np.linalg.norm(flow, axis=2)\n",
    "        mags.append(mag)\n",
    "        max_mag = max(max_mag, float(mag.max()))\n",
    "    \n",
    "    max_mag = max(max_mag, 1e-3)\n",
    "    hist_accum = np.zeros(bins, dtype=np.float32)\n",
    "    for mag in mags:\n",
    "        hist, _ = np.histogram(mag, bins=bins, range=(0, max_mag))\n",
    "        hist_accum += hist.astype(np.float32) / mag.size\n",
    "        \n",
    "    return hist_accum / len(mags)\n",
    "\n",
    "def fft_sideband_features(frames: List[np.ndarray], fs: int, f1: float) -> np.ndarray:\n",
    "    \"\"\"Compute FFT sideband features.\"\"\"\n",
    "    if not frames:\n",
    "        return np.zeros(3, dtype=np.float32)\n",
    "    \n",
    "    # Extract mean intensity signal\n",
    "    signal = [cv2.cvtColor(f, cv2.COLOR_RGB2GRAY).mean() for f in frames]\n",
    "    signal = np.array(signal, dtype=np.float32)\n",
    "    \n",
    "    # Compute FFT\n",
    "    freqs = np.fft.rfftfreq(len(signal), d=1.0/fs)\n",
    "    spectrum = np.abs(np.fft.rfft(signal))\n",
    "    \n",
    "    # Find amplitudes at fundamental and second harmonic\n",
    "    def amp_at(freq):\n",
    "        idx = np.argmin(np.abs(freqs - freq))\n",
    "        return float(spectrum[idx])\n",
    "    \n",
    "    A1 = amp_at(f1)\n",
    "    A2 = amp_at(2 * f1)\n",
    "    ratio = A2 / A1 if A1 > 1e-8 else 0.0\n",
    "    \n",
    "    return np.array([A1, A2, ratio], dtype=np.float32)\n",
    "\n",
    "def build_feature_vector(frames: List[np.ndarray], fs: int, f1: float) -> np.ndarray:\n",
    "    \"\"\"Combine all features into one vector.\"\"\"\n",
    "    energy = energy_features(frames)\n",
    "    flow = flow_hist_features(frames)\n",
    "    fft = fft_sideband_features(frames, fs, f1)\n",
    "    return np.concatenate([energy, flow, fft])\n",
    "\n",
    "# Test feature extraction on our sample frames\n",
    "feature_vector = build_feature_vector(magnified, fs=30, f1=1.0)\n",
    "print(\"Feature vector shape:\", feature_vector.shape)\n",
    "print(\"\\nFeature breakdown:\")\n",
    "print(\"- Energy features:\", feature_vector[:3])\n",
    "print(\"- Flow histogram:\", feature_vector[3:-3])\n",
    "print(\"- FFT features:\", feature_vector[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5aed8",
   "metadata": {},
   "source": [
    "## 10. Process Training Data\n",
    "\n",
    "Extract features from normal and abnormal videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path: str, segment_frames: int = 150, step_frames: int = 75,\n",
    "                fs: int = 30, f1: float = 1.0) -> List[np.ndarray]:\n",
    "    \"\"\"Process video and extract features from segments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    video_path : str\n",
    "        Path to video file\n",
    "    segment_frames : int\n",
    "        Number of frames per segment\n",
    "    step_frames : int\n",
    "        Number of frames to shift between segments\n",
    "    fs : int\n",
    "        Frame rate in Hz\n",
    "    f1 : float\n",
    "        Fundamental frequency for FFT features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[np.ndarray]\n",
    "        List of feature vectors\n",
    "    \"\"\"\n",
    "    print(f\"Processing {os.path.basename(video_path)}...\")\n",
    "    \n",
    "    # Extract frames\n",
    "    frames = extract_frames(video_path)\n",
    "    print(f\"Extracted {len(frames)} frames\")\n",
    "    \n",
    "    # Stabilize\n",
    "    frames = stabilise_frames(frames)\n",
    "    print(\"Stabilization complete\")\n",
    "    \n",
    "    # Crop ROI\n",
    "    frames = crop_roi(frames)\n",
    "    print(f\"ROI cropping complete, shape: {frames[0].shape}\")\n",
    "    \n",
    "    # Process segments\n",
    "    features = []\n",
    "    for start in tqdm(range(0, len(frames) - segment_frames, step_frames),\n",
    "                     desc=\"Processing segments\"):\n",
    "        segment = frames[start:start + segment_frames]\n",
    "        \n",
    "        # Magnify motion\n",
    "        magnified = magnify_motion(segment, fs, f1-0.5, f1+0.5, alpha=10)\n",
    "        \n",
    "        # Extract features\n",
    "        feat = build_feature_vector(magnified, fs, f1)\n",
    "        features.append(feat)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Process normal and abnormal videos\n",
    "X_normal = process_video(NORMAL_VIDEO)\n",
    "y_normal = np.zeros(len(X_normal))\n",
    "\n",
    "X_abnormal = process_video(ABNORMAL_VIDEO)\n",
    "y_abnormal = np.ones(len(X_abnormal))\n",
    "\n",
    "# Combine datasets\n",
    "X = np.vstack([X_normal, X_abnormal])\n",
    "y = np.concatenate([y_normal, y_abnormal])\n",
    "\n",
    "print(\"\\nDataset summary:\")\n",
    "print(f\"Normal samples: {len(X_normal)}\")\n",
    "print(f\"Abnormal samples: {len(X_abnormal)}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Save processed data\n",
    "np.save('features.npy', X)\n",
    "np.save('labels.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f081e6",
   "metadata": {},
   "source": [
    "## 11. Train and Evaluate Classifier\n",
    "\n",
    "Train SVM classifier with hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83154bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from joblib import dump, load\n",
    "\n",
    "# Split data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Optionally apply PCA (recommended for high-dimensional features)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "\n",
    "# SVM with grid search\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "svc = SVC(probability=True)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid = GridSearchCV(svc, param_grid, cv=cv, n_jobs=-1)\n",
    "grid.fit(X_train_pca, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "dump(best_model, 'svm_model.joblib')\n",
    "\n",
    "# Evaluate\n",
    "y_pred = best_model.predict(X_val_pca)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "prec = precision_score(y_val, y_pred)\n",
    "rec = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {acc:.3f}')\n",
    "print(f'Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
